import numpy as np
import pandas as pd
import itertools

from sklearn.svm import SVC
import pennylane as qml

def load_data(filename, dataset_index):
    """Load data from files as generated by Tom's code
    """
    data = pd.read_pickle(filename)
    X, Y = data.iloc[dataset_index]
    return X, Y

def optimize_kernel_param(
    kernel,
    X,
    y,
    init_param,
    samples=None,
    seed=None,
    optimizer=qml.AdamOptimizer,
    optimizer_kwargs={'stepsize':0.2},
    N_epoch=20,
    verbose=5,
    use_manual_grad=False,
    dx=1e-6,
    atol=1e-3,
):
    """Optimize the variational parameters of a trainable kernel
    Args:
      kernel (qml.kern.EmbeddingKernel): The variational kernel to train
      X (ndarray): Training feature vectors
      y (ndarray): Training labels
      init_param (ndarray): Initial choice of variational parameter
      samples (int): Number of samples to take from the training data
      seed (int): Seed for random sampling of training data
      optimizer (qml.GradientDescentOptimizer): Optimizer to use
      optimizer_kwargs (dict): Keyword arguments for the optimizer
      N_epoch (int): Number of epochs to run the optimizer for
      verbose (int): Will print cost function every verbose steps
      use_manual_grad (bool): Whether or not to use the manually finite difference gradient
      dx (float): Finite difference shift for manual gradient
      atol (float): Interrupt precision for the optimizer
        If the change in cost function drops below atol, it interrupts
    Returns:
      opt_param (ndarray): Optimal choice of variational kernel parameters
      opt_cost (float): Target alignment at opt_param
    """
    opt = optimizer(**optimizer_kwargs)
    param = np.copy(init_param)

    last_cost = 1e10
    opt_param = None
    opt_cost = None
    for i in range(N_epoch):
        x, y = sample_data(X, y, samples, seed)
        cost_fn = lambda param: -kernel.target_alignment(x, y, param)
        if use_manual_grad:
            grad_fn = lambda param: (
                -target_alignment_grad(x, y, kernel, kernel_param=param, dx=dx),
            )
        else:
            grad_fn = None
        current_cost = -cost_fn(param)
        if i%verbose==0:
            print(f"At iteration {i} the target alignment is {current_cost} (params={param})")
        if current_cost<last_cost:
            opt_param = param.copy()
            opt_cost = np.copy(-current_cost)
        if np.abs(last_cost-current_cost)<atol:
            break
        param = opt.step(cost_fn, param, grad_fn=grad_fn)
        last_cost = current_cost

    return opt_param, opt_cost

def train_svm(kernel, X_train, y_train, param):
    """Train a Support vector machine with a given (trainable) kernel
    Args:
      kernel (qml.kern.EmbeddingKernel): Kernel to use in the SVM
      X_train (ndarray): Training feature vectors
      y_train (ndarray): Training labels
      param (ndarray): Variational parameters for the kernel
    Returns:
      svm (sklearn model): Trained SVM instance
    """
    def kernel_matrix(A, B):
        """Compute the matrix whose entries are the kernel
           evaluated on pairwise data from sets A and B.
           If A is equal to B, use the optimized computation in qml.kern."""
        if A.shape==B.shape and np.allclose(A, B):
            return kernel.square_kernel_matrix(A, param)
        return kernel.kernel_matrix(A, B, param)
    
    svm = SVC(kernel=kernel_matrix).fit(X_train, y_train)
    return svm
    
def validate(model, X, y_true):
    """Run a prediction given a model and evaluate its classification performance.
    Args:
      model (sklearn model): Trained model instance
      X (ndarray): Test feature vectors to classify
      y_true (ndarray): Correct labels of test data
    Returns:
      prec (float): Classification precision
    """
    n_test = len(y_true)
    y_pred = model.predict(X)
    errors = np.sum(np.abs((y_true - y_pred)/2))
    prec = (n_test-errors)/n_test

    return prec

def sample_data(X, y, samples=None, seed=None):
    """Sample a subset of a given data set
    Args:
      X (ndarray): Feature vectors
      y (ndarray): Labels
      samples (int): Number of samples to draw. Return full data if None
      seed (int): Random seed for sampling.
    Returns:
      X (ndarray): Subset of feature vectors
      y (ndarray): Subset of labels
    """

    m = len(y)
    if samples is None or samples>m:
        return X, y
    else:
        if seed is None:
            seed = np.random.randint(0, 1000000)
        np.random.seed(seed)
        sampled = np.random.choice(list(range(m)), samples)
        X = X[sampled]
        y = y[sampled]
    return X, y

def target_alignment_grad(X, y, kernel, kernel_param, dx=1e-6):
    """Compute the finite difference gradient of the target alignment w.r.t 
    variational kernel parameters.
    Args:
      X (ndarray): Feature vectors
      y (ndarray): Labels
      kernel (qml.kern.EmbeddingKernel): Kernel to use
      kernel_param (ndarray): Variational parameters for trainable kernel at which to compute grad
      dx (float): Finite difference shift
    Returns:
      g (ndarray): Gradient of target alignment of kernel at kernel_param
    """
    g = np.zeros_like(kernel_param)
    shifts = np.eye(len(kernel_param))*dx/2
    for i, shift in enumerate(shifts):
        ta_plus = kernel.target_alignment(X, y, kernel_param+shift)
        ta_minus = kernel.target_alignment(X, y, kernel_param-shift)
        g[i] = (ta_plus-ta_minus)/dx
    return g

def gen_cubic_data(index):
    """Deterministically (!) generate a dataset with feature vectors on a (3D) 
    cube and random balanced labels.
    Args:
      index (int): We count through all distinct labelings of the cube (removing 
        those that are equivalent due to symmetries) in a deterministic fashion, 
        index determines the set that is returned and has to be in range(17).
    Returns:
      X (ndarray): Feature vectors
      y (ndarray): Labels
    """
    # Generate cube vertices
    feature_values = [0.1, np.pi-0.1]
    X = np.array(list(itertools.product(feature_values, repeat=3)))

    # Symmetry transformations of cube
    sym_indices = [
        [4,5,6,7,0,1,2,3],
        [3,2,1,0,7,6,5,4],
        [1,0,3,2,5,4,7,6],
        [1,2,3,0,5,6,7,4],
        [2,3,0,1,6,7,4,5],
        [3,0,1,2,7,4,5,6],
        [1,5,6,2,0,4,7,3],
        [5,4,7,6,1,0,3,2],
        [4,0,3,7,5,1,2,6],
        [3,2,6,7,0,1,5,4],
        [7,6,5,4,3,2,1,0],
        [4,5,1,0,7,6,2,3],
    ]
    # all possible balanced label assignments
    unique = set(itertools.permutations('00001111'))
    new = set()
    # Filter for duplicates under symmetries
    for el in unique:
        for ind in sym_indices:
            _new = tuple(el[i] for i in ind)
            if _new in new:
                break
        else:
            new.add(''.join(el))
    y = np.array([-1 if v=='0' else 1 for v in sorted(list(new))[index]])
    return X, y

def gen_hypercubic_data(n_dim, n_samples, seed=None):
    """Generate a dataset with feature vectors on a (n-dimensional) hypercube
    and random balanced labels. This is NOT deterministic.
    Args:
      n_dim (int): Number of dimensions of the hypercube, corresponds to number of features
      n_samples (int): Number of data points to sample
      seed (int): Random seed for sampling of hypercube vertices and for shuffling of labels
    Returns:
      X (ndarray): Feature vectors
      y (ndarray): Labels
    """
    if n_samples%2!=0:
        raise ValueError(f"Please give an even number of samples, got {n_samples}")
    # Generate cube vertices
    feature_values = [0.1, np.pi-0.1]
    np.random.seed(seed)
    X_indices = np.random.choice(range(2**n_dim), n_samples)
    X = np.array(list(itertools.product(feature_values, repeat=n_dim)))[X_indices]
    
    y = np.array([-1]*(n_samples//2)+[1]*(n_samples//2))
    np.random.shuffle(y)

    return X, y
