{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantum Embedding Kernels for MNIST with floq\n",
    "\n",
    "_Authors: Peter-Jan Derks, Paul FÃ¤hrmann, Elies Gil-Fuster, Tom Hubregtsen, Johannes Jakob Meyer and David Wierichs_\n",
    "\n",
    "In this demonstration we showcase how simple it is to use the `qml.kernels` module to perform classification with large datapoints using `floq` to offload the heavy computation of wide circuits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "\n",
    "We will use the popular MNIST dataset, consisting of tens of thousands of $28 \\times 28$ pixel images. To make this tractable for simulation on our hardware, we will only work with a small subset of MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import remote_cirq\n",
    "\n",
    "np.random.seed(2658)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we will use `keras` for loading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(train_X, train_y), (test_X, test_y) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now extract the first 10 zeros and the first 10 ones as our training data. We will also rescale the images, whos pixel values are given with values in the interval $[0, 255]$, to the interval $[0, \\pi]$ to be compatible with embeddings that use angles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_X.shape)\n",
    "\n",
    "sample_size = 5\n",
    "\n",
    "train_idx0 = np.argwhere(train_y == 0)[:sample_size]\n",
    "train_X0 = train_X[train_idx0].squeeze() * np.pi/255\n",
    "\n",
    "train_idx1 = np.argwhere(train_y == 1)[:sample_size]\n",
    "train_X1 = train_X[train_idx1].squeeze() * np.pi/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us have a look at our training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = mpl.gridspec.GridSpec(2, sample_size) \n",
    "fig = plt.figure(figsize=(16,4))\n",
    "\n",
    "for j in range(sample_size):\n",
    "    ax=plt.subplot(gs[0, j])\n",
    "    plt.imshow(train_X0[j], cmap=plt.get_cmap('gray'))\n",
    "    ax.axis(\"off\")\n",
    "    \n",
    "    ax=plt.subplot(gs[1, j])\n",
    "    plt.imshow(train_X1[j], cmap=plt.get_cmap('gray'))\n",
    "    ax.axis(\"off\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the zeros and ones extracted, we can now create the actual variables we use for the training of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.vstack([train_X0, train_X1])\n",
    "y = np.hstack([[-1]*sample_size, [1]*sample_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Quantum Embedding Kernel\n",
    "\n",
    "PennyLane's `kernels` module allows for a particularly simple implementation of Quantum Embedding Kernels. The first ingredient we need for this is an _ansatz_ that represents the unitary $U(\\boldsymbol{x})$ we use for embedding the data into a quantum state. We will use a structure where a single layer is repeated multiple times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer(x, params, wires, i0=0, inc=1):\n",
    "    i = i0\n",
    "    for j, wire in enumerate(wires):\n",
    "        qml.Hadamard(wires=[wire])\n",
    "        qml.RZ(x[i % len(x)], wires=[wire])\n",
    "        i += inc\n",
    "        qml.RY(params[0, j], wires=[wire])\n",
    "        \n",
    "    qml.broadcast(unitary=qml.CRZ, pattern=\"ring\", wires=wires, parameters=params[1])\n",
    "\n",
    "@qml.template\n",
    "def ansatz(x, params, wires):\n",
    "    for j, layer_params in enumerate(params):\n",
    "        layer(x, layer_params, wires, i0=j * len(wires))\n",
    "        \n",
    "def random_params(num_wires, num_layers):\n",
    "    return np.random.uniform(0, 2*np.pi, (num_layers, 2, num_wires))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now in a place where we can create the embedding. Together with the ansatz we only need a device to run the quantum circuit on. For the purposes of this tutorial we will use the `floq` device with $28$ wires. Note that we need to flatten the input data to our ansatz, as the ansatz expects a flat array but the datapoints are two dimensional images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_WIRES = 26 # can also be 28 x 28\n",
    "N_LAYERS = 31\n",
    "\n",
    "API_KEY = \"YOUR KEY\"\n",
    "sim = remote_cirq.RemoteSimulator(API_KEY)\n",
    "dev = qml.device(\"cirq.simulator\",\n",
    "                 wires=N_WIRES,\n",
    "                 simulator=sim,\n",
    "                 analytic=False)\n",
    "\n",
    "wires = list(range(N_WIRES))\n",
    "k = qml.kernels.EmbeddingKernel(lambda x, params: ansatz(x.flatten(), params, wires), dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this was all of the magic! The `EmbeddingKernel` class took care of providing us with a circuit that calculates the overlap. Before we can take a look at the kernel values we have to provide values for the variational parameters. We will initialize them such that the ansatz circuit has $28$ layers to be able to capture the full MNIST image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_params = random_params(N_WIRES, N_LAYERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can have a look at the kernel value between the first and the second datapoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The kernel value between the first and second datapoint is {:.3f}\".format(k(train_X0[0], train_X1[0], init_params)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mutual kernel values between all elements of the dataset form the _kernel matrix_. We can calculate it via the `square_kernel_matrix` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = k.square_kernel_matrix(X, init_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Quantum Embedding Kernel for predictions\n",
    "\n",
    "The quantum kernel alone can not be used to make predictions on a dataset, becaues it essentially just a tool to measure the similarity between two datapoints. To perform an actual prediction we will make use of scikit-learns support vector classifier (SVC). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will compute the kernel matrix needed for the `SVC` by hand, which is why we have to put `kernel=\"precomputed\"` as the argument. Note that this does not train the parameters in our circuit but it trains the SVC on the kernel matrix with the given labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel=\"precomputed\").fit(K_init, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see how well our classifier performs we will measure what percentage of the training set it classifies correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The accuracy of a kernel with random parameters on the training set is {:.3f}\".format(\n",
    "    1 - np.count_nonzero(svm.predict(K_init) - y) / len(y))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating performance on test data\n",
    "\n",
    "Now we will compare this to the performance on unseen data. To this end, we extract the next ten zeros and ones from the MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idx0 = np.argwhere(train_y == 0)[10:10+sample_size]\n",
    "test_X0 = train_X[train_idx0].squeeze() * np.pi/255\n",
    "\n",
    "test_idx1 = np.argwhere(train_y == 1)[10:10+sample_size]\n",
    "test_X1 = train_X[train_idx1].squeeze() * np.pi/255\n",
    "\n",
    "X_test = np.vstack([test_X0, test_X1])\n",
    "y_test = np.hstack([[-1]*sample_size, [1]*sample_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make a prediction, we have to compute the kernels between the test datapoints and the training datapoints. The `EmbeddingKernel` class offers a convenience method for this in the form of the `kernel_matrix` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_pred = k.kernel_matrix(X_test, X, init_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check how the kernel performs on the unseen data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The test accuracy of a kernel with random parameters is {:.3f}\".format(\n",
    "    1 - np.count_nonzero(svm.predict(K_pred) - y_test) / len(y_test))\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light",
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.5",
    "jupytext_version": "1.10.2"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
