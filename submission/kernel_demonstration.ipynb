{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "stopped-directory",
   "metadata": {},
   "source": [
    "**To be able to run this notebook you need to install the modified PennyLane version that contains the `qml.kernels` module via**\n",
    "```\n",
    "pip install git+https://www.github.com/johannesjmeyer/pennylane@kernel_module --upgrade\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-couple",
   "metadata": {},
   "source": [
    "# Quantum Embedding Kernels with PennyLane's kernels module\n",
    "\n",
    "_Authors: Peter-Jan Derks, Paul FÃ¤hrmann, Elies Gil-Fuster, Tom Hubregtsen, Johannes Jakob Meyer and David Wierichs_\n",
    "_On Feb 26th 2021_\n",
    "\n",
    "Kernel methods are one of the cornerstones of classical machine learning.\n",
    "To understand what a kernel method does let's first revisit one of the simplest methods to assign binary labels to datapoints: linear classification.\n",
    "\n",
    "Imagine we want to discern two different classes of points that lie in different corners of the plane.\n",
    "A linear classifier corresponds to drawing a line and assigning different labels to the regions on opposing sides of the line:\n",
    "\n",
    "<img src=\"linear_classification.png\" alt=\"Linear classification.\" width=\"300\"/>\n",
    "\n",
    "We can mathematically formalize this by assigning the label $y$ via\n",
    "\n",
    "$$\n",
    "y(\\boldsymbol{x}) = \\operatorname{sgn}(\\langle \\boldsymbol{w}, \\boldsymbol{x}\\rangle + b).\n",
    "$$\n",
    "\n",
    "The vector $\\boldsymbol{w}$ points perpendicular to the line and thus determine its slope.\n",
    "The independent term $b$ specificies the position on the plane.\n",
    "In this form, linear classification can also be extended to higher dimensional vectors $\\boldsymbol{x}$, where a line does not divide the entire space into two regions anymore.\n",
    "Instead one needs a _hyperplane_.\n",
    "It is immediately clear that this method is not very powerful, as datasets that are not separable by a hyperplane can't be treated. \n",
    "\n",
    "We can actually sneak around this limitation by performing a neat trick: if we define some map $\\phi(\\boldsymbol{x})$ that _embeds_ our datapoints into a larger _feature space_ and then perform linear classification there, we could actually realise non-linear classification in our original space!\n",
    "\n",
    "<img src=\"embedding_nonlinear_classification.png\" alt=\"Linear classification with embedding\" width=\"660\"/>\n",
    "\n",
    "If we go back to the expression for our prediction and include the embedding, we get\n",
    "\n",
    "$$\n",
    "y(\\boldsymbol{x}) = \\operatorname{sgn}(\\langle \\boldsymbol{w}, \\phi(\\boldsymbol{x})\\rangle + b).\n",
    "$$\n",
    "\n",
    "We will forgo one tiny step, but it can be shown that for the purposes of optimal classification, we can choose the vector defining the decision boundary as a linear combination of the embedded datapoints $\\boldsymbol{w} = \\sum_i \\alpha_i \\phi(\\boldsymbol{x}_i)$. Putting this into the formula yields\n",
    "\n",
    "$$\n",
    "y(\\boldsymbol{x}) = \\operatorname{sgn}\\left(\\sum_i \\alpha_i \\langle \\phi(\\boldsymbol{x}_i), \\phi(\\boldsymbol{x})\\rangle + b\\right).\n",
    "$$\n",
    "\n",
    "This rewriting might not seem useful at first, but notice the above formula only contains inner products between vectors in the embedding space:\n",
    "\n",
    "$$\n",
    "k(\\boldsymbol{x}, \\boldsymbol{y}) = \\langle \\phi(\\boldsymbol{x}), \\phi(\\boldsymbol{y})\\rangle.\n",
    "$$\n",
    "\n",
    "We call this function the _kernel_.\n",
    "The clue now is that we can often find an explicit formula for the kernel $k$ that makes it superfluous to actually perform the embedding $\\phi$.\n",
    "Consider for example the following embedding and the associated kernel:\n",
    "\n",
    "$$\n",
    "\\phi((x_1, x_2)) = (x_1^2, \\sqrt{2} x_1 x_2, x_2^2) \\qquad\n",
    "k(\\boldsymbol{x}, \\boldsymbol{y}) = x_1^2 y_1^2 + 2 x_1 x_2 y_1 y_2 + x_2^2 y_2^2 = \\langle \\boldsymbol{x}, \\boldsymbol{y} \\rangle^2\n",
    "$$\n",
    "\n",
    "This means by just replacing the regular scalar product in our linear classification with the map $k$, we can actually express much more intricate decision boundaries!\n",
    "\n",
    "This is very important, because in many interesting cases the embedding will be much more costlier to compute than the kernel.\n",
    "\n",
    "In this demonstration, we will explore one particular kind of kernel that can be realized on near-term quantum computers, namely _Quantum Embedding Kernels (QEKs)_.\n",
    "These are kernels that arise from embedding data into the space of quantum states.\n",
    "We formalize this by considering a parameterised quantum circuit $U(\\boldsymbol{x})$ that embeds datapoint $\\boldsymbol{x}$ onto the state\n",
    "\n",
    "$$\n",
    "|\\psi(\\boldsymbol{x})\\rangle = U(\\boldsymbol{x}) |0 \\rangle.\n",
    "$$\n",
    "\n",
    "The kernel value is then given by the _overlap_ of the associated embedded quantum states\n",
    "\n",
    "$$\n",
    "k(\\boldsymbol{x}, \\boldsymbol{y}) = | \\langle\\psi(\\boldsymbol{x})|\\psi(\\boldsymbol{y})\\rangle|^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-negotiation",
   "metadata": {},
   "source": [
    "## A toy problem\n",
    "\n",
    "In this demonstration, we will treat a toy problem that showcases the inner workings of our approach. \n",
    "We of course need to start with some imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-jacksonville",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(2658)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "involved-jamaica",
   "metadata": {},
   "source": [
    "And we proceed right away to create the `DoubleCake` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advance-armstrong",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleCake:\n",
    "    def _make_circular_data(self): \n",
    "        \"\"\"Generate datapoints arranged in an even circle.\"\"\"\n",
    "        center_indices = np.array(range(0, self.num_sectors))\n",
    "        sector_angle = 2*np.pi / self.num_sectors\n",
    "        angles = (center_indices + 0.5) * sector_angle\n",
    "        x = 0.7 * np.cos(angles)\n",
    "        y = 0.7 * np.sin(angles)\n",
    "        labels = 2 * np.remainder(np.floor_divide(angles, sector_angle), 2)- 1 \n",
    "        \n",
    "        return x, y, labels\n",
    "\n",
    "    def __init__(self, num_sectors):\n",
    "        self.num_sectors = num_sectors\n",
    "        \n",
    "        x1, y1, labels1 = self._make_circular_data()\n",
    "        x2, y2, labels2 = self._make_circular_data()\n",
    "\n",
    "        # x and y coordinates of the datapoints\n",
    "        self.x = np.hstack([x1, .5 * x2])\n",
    "        self.y = np.hstack([y1, .5 * y2])\n",
    "        \n",
    "        # Canonical form of dataset\n",
    "        self.X = np.vstack([self.x, self.y]).T\n",
    "        \n",
    "        self.labels = np.hstack([labels1, -1 * labels2])\n",
    "        \n",
    "        # Canonical form of labels\n",
    "        self.Y = self.labels.astype(int)\n",
    "\n",
    "    def plot(self, ax, show_sectors=False):\n",
    "        ax.scatter(self.x, self.y, c=self.labels, cmap=mpl.colors.ListedColormap(['#FF0000', '#0000FF']), s=25, marker='s')\n",
    "        sector_angle = 360/self.num_sectors\n",
    "        \n",
    "        if show_sectors:\n",
    "            for i in range(self.num_sectors):\n",
    "                color = ['#FF0000', '#0000FF'][(i % 2)]\n",
    "                other_color = ['#FF0000', '#0000FF'][((i + 1) % 2)]\n",
    "                ax.add_artist(mpl.patches.Wedge((0, 0), 1, i * sector_angle, (i+1)*sector_angle, lw=0, color=color, alpha=0.1, width=.5))\n",
    "                ax.add_artist(mpl.patches.Wedge((0, 0), .5, i * sector_angle, (i+1)*sector_angle, lw=0, color=other_color, alpha=0.1))\n",
    "                ax.set_xlim(-1, 1)\n",
    "\n",
    "        ax.set_ylim(-1, 1)\n",
    "        ax.set_aspect(\"equal\")\n",
    "        ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excess-boundary",
   "metadata": {},
   "source": [
    "Let's now have a look at our dataset. In our example, we will work with 6 sectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aquatic-congo",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DoubleCake(6)\n",
    "\n",
    "dataset.plot(plt.gca(), show_sectors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-montgomery",
   "metadata": {},
   "source": [
    "## Defining a Quantum Embedding Kernel\n",
    "\n",
    "PennyLane's `kernels` module allows for a particularly simple implementation of Quantum Embedding Kernels.\n",
    "The first ingredient we need for this is an _Ansatz_ that represents the unitary $U(\\boldsymbol{x})$ we use for embedding the data into a quantum state.\n",
    "We will use a structure where a single layer is repeated multiple times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "better-crisis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer(x, params, wires, i0=0, inc=1):\n",
    "    \"\"\"Building block of the embedding Ansatz\"\"\"\n",
    "    i = i0\n",
    "    for j, wire in enumerate(wires):\n",
    "        qml.Hadamard(wires=[wire])\n",
    "        qml.RZ(x[i % len(x)], wires=[wire])\n",
    "        i += inc\n",
    "        qml.RY(params[0, j], wires=[wire])\n",
    "        \n",
    "    qml.broadcast(unitary=qml.CRZ, pattern=\"ring\", wires=wires, parameters=params[1])\n",
    "\n",
    "@qml.template\n",
    "def ansatz(x, params, wires):\n",
    "    \"\"\"The embedding Ansatz\"\"\"\n",
    "    for j, layer_params in enumerate(params):\n",
    "        layer(x, layer_params, wires, i0=j * len(wires))\n",
    "        \n",
    "def random_params(num_wires, num_layers):\n",
    "    return np.random.uniform(0, 2*np.pi, (num_layers, 2, num_wires))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "curious-greensboro",
   "metadata": {},
   "source": [
    "We are now in a place where we can create the embedding.\n",
    "Together with the Ansatz we only need a device to run the quantum circuit on.\n",
    "For the purposes of this tutorial we will use PennyLane's `default.qubit` device with 5 wires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "palestinian-update",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev = qml.device(\"default.qubit\", wires=5)\n",
    "wires = list(range(5))\n",
    "# compute the kernel matrix\n",
    "k = qml.kernels.EmbeddingKernel(lambda x, params: ansatz(x, params, wires), dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-capability",
   "metadata": {},
   "source": [
    "And this was all of the magic!\n",
    "The `EmbeddingKernel` class took care of providing us with a circuit that calculates the overlap.\n",
    "Before focusing on the kernel values we have to provide values for the variational parameters.\n",
    "At this point we fix the number of layers in the Ansatz circuit to $6$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-whale",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_params = random_params(5, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-schedule",
   "metadata": {},
   "source": [
    "Now we can have a look at the kernel value between the first and the second datapoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-punch",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The kernel value between the first and second datapoint is {:.3f}\".format(k(dataset.X[0], dataset.X[1], init_params)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preliminary-cement",
   "metadata": {},
   "source": [
    "The mutual kernel values between all elements of the dataset form the _kernel matrix_.\n",
    "We can inspect it via the `square_kernel_matrix` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "described-pantyhose",
   "metadata": {},
   "outputs": [],
   "source": [
    "K_init = k.square_kernel_matrix(dataset.X, init_params)\n",
    "\n",
    "with np.printoptions(precision=3, suppress=True):\n",
    "    print(K_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-departure",
   "metadata": {},
   "source": [
    "## Using the Quantum Embedding Kernel for predictions\n",
    "\n",
    "The quantum kernel alone can not be used to make predictions on a dataset, becaues it is essentially just a tool to measure the similarity between two datapoints.\n",
    "To perform an actual prediction we will make use of scikit-learn's Support Vector Classifier (SVC). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desirable-horror",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "willing-robin",
   "metadata": {},
   "source": [
    "The `SVC` class expects a function that maps two sets of datapoints to the corresponding kernel matrix.\n",
    "This is provided by the `kernel_matrix` property of the `EmbeddingKernel` class, so we only need to use a lambda construction to include our parameters.\n",
    "Once we have this, we can let scikit adjust the SVM from our Quantum Embedding Kernel.\n",
    "\n",
    "Note this step does not modify the free parameters in our circuit Ansatz.\n",
    "What it does is solving a different optimization task for the $\\alpha$ and $b$ vectors we introduced above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-adrian",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(kernel=lambda X1, X2: k.kernel_matrix(X1, X2, init_params)).fit(dataset.X, dataset.Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-cement",
   "metadata": {},
   "source": [
    "To see how well our classifier performs we will measure what percentage it classifies correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-scottish",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(classifier, X, Y_target):\n",
    "    return 1 - np.count_nonzero(classifier.predict(X) - Y_target) / len(Y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-reviewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The accuracy of a kernel with random parameters is {:.3f}\".format(accuracy(svm, dataset.X, dataset.Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retained-ending",
   "metadata": {},
   "source": [
    "We are also interested in seeing how the decision boundaries in this classification look like.\n",
    "This could help us spotting overfitting issues visually in more complex data sets.\n",
    "To this end we will introduce a second helper method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "requested-scanner",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundaries(classifier, ax, N_gridpoints=14):\n",
    "    _xx, _yy = np.meshgrid(np.linspace(-1, 1, N_gridpoints), np.linspace(-1, 1, N_gridpoints))\n",
    "\n",
    "    _zz = np.zeros_like(_xx)\n",
    "    for idx in np.ndindex(*_xx.shape):\n",
    "        _zz[idx] = classifier.predict(np.array([_xx[idx], _yy[idx]])[np.newaxis,:])\n",
    "\n",
    "    plot_data = {'_xx' : _xx, '_yy' : _yy, '_zz' : _zz}\n",
    "    ax.contourf(_xx, _yy, _zz, cmap=mpl.colors.ListedColormap(['#FF0000', '#0000FF']), alpha=.2, levels=[-1, 0,  1])            \n",
    "    dataset.plot(ax)\n",
    "    \n",
    "    return plot_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-preference",
   "metadata": {},
   "source": [
    "With that done, let's have a look at the decision boundaries for our initial classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continuous-mechanism",
   "metadata": {},
   "outputs": [],
   "source": [
    "init_plot_data = plot_decision_boundaries(svm, plt.gca())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confident-haven",
   "metadata": {},
   "source": [
    "We see the outer points in the dataset can be correctly classified, but we still struggle with the inner circle.\n",
    "But remember we have a circuit with many free parameters!\n",
    "It is reasonable to believe we can give values to those parameters which improve the overall accuracy of our SVC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upset-trailer",
   "metadata": {},
   "source": [
    "## Training the Quantum Embedding Kernel\n",
    "\n",
    "To be able to train the Quantum Embedding Kernel we need some measure of how well it fits the dataset in question.\n",
    "Performing an exhaustive search in parameter space is not a good solution because it is very resource intensive, and since the accuracy is a discrete quantity we would not be able to detect small improvements. \n",
    "\n",
    "We can, however, resort to a more specialized measure, the _kernel-target alignment_ [1].\n",
    "The kernel-target alignment compares the similarity predicted by the quantum kernel to the actual labels of the training data.\n",
    "It is based on _kernel alignment_, a similiarity measure between two kernels with given kernel matrices $K_1$ and $K_2$:\n",
    "\n",
    "$$\n",
    "\\operatorname{KA}(K_1, K_2) = \\frac{\\operatorname{Tr}(K_1 K_2)}{\\sqrt{\\operatorname{Tr}(K_1^2)\\operatorname{Tr}(K_2^2)}}\n",
    "$$\n",
    "\n",
    "Seen from a more theoretical side, this is nothing else than the cosine of the angle between the kernel matrices $K_1$ and $K_2$ seen as vectors in the space of matrices with the Hilbert-Schmidt- (or Frobenius-) scalar product $\\langle A, B \\rangle = \\operatorname{Tr}(A^T B)$.\n",
    "This reinforces the geometric picture of how this measure relates to objects being aligned in a vector space.\n",
    "\n",
    "The training data enters the picture by defining a kernel that expresses the labelling in the vector $\\boldsymbol{y}$ by assigning the product of the respective labels as the kernel function\n",
    "\n",
    "$$\n",
    "k_{\\boldsymbol{y}}(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = y_i y_j\n",
    "$$\n",
    "\n",
    "The assigned kernel is thus $+1$ if both datapoints lie in the same class and $-1$ otherwise.\n",
    "The kernel matrix for this new kernel is simply given by the outer product $\\boldsymbol{y}\\boldsymbol{y}^T$.\n",
    "The kernel-target alignment is then defined as the alignment of the kernel matrix generated by the quantum kernel and $\\boldsymbol{y}\\boldsymbol{y}^T$:\n",
    "\n",
    "$$\n",
    "    \\operatorname{KTA}_{\\boldsymbol{y}}(K) \n",
    "    = \\frac{\\operatorname{Tr}(K \\boldsymbol{y}\\boldsymbol{y}^T)}{\\sqrt{\\operatorname{Tr}(K^2)\\operatorname{Tr}((\\boldsymbol{y}\\boldsymbol{y}^T)^2)}} \n",
    "    = \\frac{\\boldsymbol{y}^T K \\boldsymbol{y}}{\\sqrt{\\operatorname{Tr}(K^2)} N}\n",
    "$$\n",
    "\n",
    "where $N$ is the number of elements in $\\boldsymbol{y}$.\n",
    "\n",
    "In summary, the kernel-target alignment effectively captures how well the kernel you chose reproduces the actual similarities of the data.\n",
    "It does have one drawback, however: having a high kernel-target alignment is only a necessary but not a sufficient condition for a good performance of the kernel [1].\n",
    "This means having good alignment is guaranteed to good performance, but optimal alignment will not always bring optimal training accuracy.\n",
    "\n",
    "Let's now come back to the actual implementation.\n",
    "PennyLane's `EmbeddingKernel` class allows you to easily evaluate the kernel target alignment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-meaning",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The kernel-target-alignment for our dataset with random parameters is {:.3f}\".format(\n",
    "    k.target_alignment(dataset.X, dataset.Y, init_params))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "social-niger",
   "metadata": {},
   "source": [
    "Now let's code up an optimization loop and improve this!\n",
    "\n",
    "We will make use of regular gradient descent optimization.\n",
    "To speed up the optimization we will not use the entire training set but rather sample smaller subsets of the data at each step, we choose $4$ datapoints at random.\n",
    "Remember that PennyLane's inbuilt optimizer works to _minimize_ the cost function that is given to it, which is why we have to multiply the kernel target alignment by $-1$ to actually _maximize_ it in the process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sealed-swedish",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = init_params\n",
    "opt = qml.GradientDescentOptimizer(2.5)\n",
    "\n",
    "for i in range(500):\n",
    "    subset = np.random.choice(list(range(len(dataset.X))), 4)\n",
    "    params = opt.step(lambda _params: -k.target_alignment(dataset.X[subset], dataset.Y[subset], _params), params)\n",
    "    \n",
    "    if (i+1) % 50 == 0:\n",
    "        print(\"Step {} - Alignment = {:.3f}\".format(i+1, k.target_alignment(dataset.X, dataset.Y, params)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varying-service",
   "metadata": {},
   "source": [
    "We want to assess the impact of training the parameters of the quantum kernel.\n",
    "Thus, let's build a second support vector classifier with the trained kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-democracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_trained = SVC(kernel=lambda X1, X2: k.kernel_matrix(X1, X2, params)).fit(dataset.X, dataset.Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-sphere",
   "metadata": {},
   "source": [
    "We expect to see an accuracy improvement vs. the SVM with random parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-pride",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The accuracy of a kernel with trained parameters is {:.3f}\".format(accuracy(svm_trained, dataset.X, dataset.Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genuine-treat",
   "metadata": {},
   "source": [
    "Very well!\n",
    "We now achieved perfect classification!\n",
    "\n",
    "Following on the results that SVM's have proven good generalisation behavior, it will be interesting to inspect the decision boundaries of our classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "visible-lender",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_plot_data = plot_decision_boundaries(svm_trained, plt.gca())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-castle",
   "metadata": {},
   "source": [
    "Indeed, we see that now not only every data instance falls within the correct class, but also that there are no strong artifacts that make us distrust the model.\n",
    "In this sense, our approach benefits from both:\n",
    "On the one hand it can adjust itself to the dataset, and on the other hand is not expected to suffer from bad generalisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suitable-oregon",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "[1] Wang, Tinghua, Dongyan Zhao, and Shengfeng Tian. \"An overview of kernel alignment and its applications.\" _Artificial Intelligence Review_ 43.2 (2015): 179-192."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:light",
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.5",
    "jupytext_version": "1.10.2"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
